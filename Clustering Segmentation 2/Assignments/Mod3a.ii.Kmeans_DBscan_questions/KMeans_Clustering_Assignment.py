# -*- coding: utf-8 -*-
"""
Created on Wed Mar 20 20:37:32 2024

@author: Lenovo
"""

'''
# K-Means Clustering Algorithm - Data Mining (Machine Learning) Unsupervised learning Algorithm

Problem Statements:
Global air travel has seen an upward trend in recent times. The maintenance of operational efficiency and maximizing profitability are crucial for airlines and airport authorities. Businesses need to optimize airline and terminal operations to enhance passenger satisfaction, improve turnover rates, and increase overall revenue. 
The airline companies with the available data want to find an opportunity to analyze and understand travel patterns, customer demand, and terminal usage.

CRISP-ML(Q) process model describes six phases:
1. Business and Data Understanding
2. Data Preparation
3. Model Building
4. Model Evaluation
5. Deployment
6. Monitoring and Maintenance

Objective: Maximize the Sales 
Constraints: Minimize the Customer Retention
Success Criteria: 
Business Success Criteria: Increase the Sales by 10% to 12% by targeting cross-selling opportunities on current customers.
ML Success Criteria: Achieve a Silhouette coefficient of at least 0.6
Economic Success Criteria: The insurance company will see an increase in revenues by at least 8% 

Data: Refer to the ‘AirTraffic_Passenger_Statistics.csv’ dataset.

'''


# #### Install the required packages if not available
# !pip install feature_engine
# !pip install sklearn_pandas

# **Importing required packages**
# import numpy as np
import pandas as pd 
import sweetviz
import matplotlib.pyplot as plt

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

from sklearn.cluster import KMeans
from sklearn import metrics
import joblib
import pickle

# **Import the data**

from sqlalchemy import create_engine, text

traff = pd.read_csv(r"C:/Users/Lenovo/Downloads/Study material/Data Science/Clustering Segmentation 2/Assignments/Data Set/Data Set (5)/AirTraffic_Passenger_Statistics.csv")

# Credentials to connect to Database
user = 'root'  # user name
pw = '1234'  # password
db = 'traff_db'  # database name
engine = create_engine(f"mysql+pymysql://{user}:{pw}@localhost/{db}")

# to_sql() - function to push the dataframe onto a SQL table.
traff.to_sql('traff_tbl', con = engine, if_exists = 'replace', chunksize = 1000, index = False)



###### To read the data from MySQL Database
sql = 'select * from traff_tbl;'


df = pd.read_sql_query(text(sql), engine.connect())

df1 = df.drop(['Operating Airline IATA Code', 'Boarding Area', 'Year', 'Month'], axis = 1)



# Data types
df1.info()

# EXPLORATORY DATA ANALYSIS (EDA) / DESCRIPTIVE STATISTICS
# ***Descriptive Statistics and Data Distribution Function***

df1.describe()

# ## We have to check unique values for categorical data 
df1["Operating Airline"].unique()
df1["Operating Airline"].unique().size

df1["Operating Airline"].value_counts()

# AutoEDA
# Automated Libraries
# # import sweetviz
# my_report = sweetviz.analyze([df1, "df1"])
# my_report.show_html('Report.html')

# Missing Data
# Checking Null Values
df1.isnull().sum()

# Segregate Numeric and Non-numeric columns
df1.info()

# **By using Mean imputation null values can be impute**
numeric_features = df1.select_dtypes(exclude = ['object']).columns
numeric_features

# Non-numeric columns
categorical_features = df1.select_dtypes(include = ['object']).columns
categorical_features

# Define Pipeline to deal with Missing data and scaling numeric columns
num_pipeline = Pipeline([('impute', SimpleImputer(strategy = 'mean')), ('scale', MinMaxScaler())])
num_pipeline

# Fit the numeric data to the pipeline. Ignoring State column
processed1 = num_pipeline.fit(df1[numeric_features]) 

# Save the pipeline
joblib.dump(processed1, 'processed1')

# Transform the data with pipeline on numeric columns to get clean data
# traff_num = pd.DataFrame(processed.transform(df1[numeric_features]), columns = numeric_features)
# traff_num


# Encoding Non-numeric fields
# **Convert Categorical data "State" to Numerical data using OneHotEncoder**

categ_pipeline = Pipeline([('OnehotEncode', OneHotEncoder(drop = 'first'))])
categ_pipeline


processed2 = categ_pipeline.fit(df1[categorical_features])

joblib.dump(processed2, 'processed2')




# Using ColumnTransfer to transform the columns of an array or pandas DataFrame. 
# This estimator allows different columns or column subsets of the input to be
# transformed separately and the features generated by each transformer will
# be concatenated to form a single feature space.
preprocess_pipeline = ColumnTransformer([('categorical', categ_pipeline, categorical_features), 
                                       ('numerical', num_pipeline, numeric_features)],
                                        remainder = 'passthrough') # Skips the transformations for remaining columns

preprocess_pipeline

# Pass the raw data through pipeline
processed3 = preprocess_pipeline.fit(df1) 


# ## Save the Imputation and Encoding pipeline
# import joblib
joblib.dump(processed3, 'processed3')

# File gets saved under current working directory
import os
os.getcwd()

# Clean and processed data for Clustering
traff = pd.DataFrame(processed3.transform(df1).toarray(), columns = list(processed3.get_feature_names_out()))
traff


# Clean data
traff.describe()

# # CLUSTERING MODEL BUILDING

# ### KMeans Clustering
# Libraries for creating scree plot or elbow curve 
# from sklearn.cluster import KMeans
# import matplotlib.pyplot as plt

###### scree plot or elbow curve ############
TWSS = []
k = list(range(2, 15))

for i in k:
    kmeans = KMeans(n_clusters = i)
    kmeans.fit(traff)
    TWSS.append(kmeans.inertia_)

TWSS

# ## Creating a scree plot to find out no.of cluster
plt.plot(k, TWSS, 'ro-'); plt.xlabel("No_of_Clusters"); plt.ylabel("total_within_SS")



# ## Using KneeLocator
List = []

for k in range(2, 15):
    kmeans = KMeans(n_clusters = k, init = "random", max_iter = 30, n_init = 10) 
    kmeans.fit(traff)
    List.append(kmeans.inertia_)

# !pip install kneed
from kneed import KneeLocator
kl = KneeLocator(range(2, 15), List, curve = 'convex')
# kl = KneeLocator(range(2, 9), List, curve='convex', direction = 'decreasing')
kl.elbow
# plt.style.use("seaborn")
plt.plot(range(2, 15), List)
plt.xticks(range(2, 15))
plt.ylabel("Interia")
plt.axvline(x = kl.elbow, color = 'r', label = 'axvline - full height', ls = '--')
plt.show() 

# Not able to detect the best K value (knee/elbow) as the line is mostly linear

# Building KMeans clustering
model = KMeans(n_clusters = 12)
yy = model.fit(traff)

# Cluster labels
model.labels_


# ## Cluster Evaluation

# **Silhouette coefficient:**  
# Silhouette coefficient is a Metric, which is used for calculating 
# goodness of clustering technique and the value ranges between (-1 to +1).
# It tells how similar an object is to its own cluster (cohesion) compared to 
# other clusters (separation).
# A score of 1 denotes the best meaning that the data point is very compact 
# within the cluster to which it belongs and far away from the other clusters.
# Values near 0 denote overlapping clusters.

# from sklearn import metrics
metrics.silhouette_score(traff, model.labels_)

# **Calinski Harabasz:**
# Higher value of CH index means cluster are well separated.
# There is no thumb rule which is acceptable cut-off value.
metrics.calinski_harabasz_score(traff, model.labels_)

# **Davies-Bouldin Index:**
# Unlike the previous two metrics, this score measures the similarity of clusters. 
# The lower the score the better the separation between your clusters. 
# Vales can range from zero and infinity
metrics.davies_bouldin_score(traff, model.labels_)

# ### Evaluation of Number of Clusters using Silhouette Coefficient Technique
from sklearn.metrics import silhouette_score

silhouette_coefficients = []

for k in range (2, 12):
    kmeans = KMeans(n_clusters = k)
    kmeans.fit(traff)
    score = silhouette_score(traff, kmeans.labels_)
    k = k
    Sil_coff = score
    silhouette_coefficients.append([k, Sil_coff])

silhouette_coefficients

sorted(silhouette_coefficients, reverse = True, key = lambda x: x[1])


# Building KMeans clustering
bestmodel = KMeans(n_clusters = 12)
result = bestmodel.fit(traff)

# ## Save the KMeans Clustering Model
# import pickle
pickle.dump(result, open('Clust_traff.pkl', 'wb'))

import os
os.getcwd()

# Cluster labels
bestmodel.labels_

mb = pd.Series(bestmodel.labels_) 

# Concate the Results with data
df_clust = pd.concat([mb, df1], axis = 1)
df_clust = df_clust.rename(columns = {0:'cluster_id'})
df_clust.head()

# Aggregate using the mean of each cluster
cluster_agg = df_clust.select_dtypes(exclude = ['object']).groupby(df_clust.cluster_id).mean()
cluster_agg

# Save the Results to a CSV file
df_clust.to_csv('KMeans_traff.csv', encoding = 'utf-8', index = False)
 
import os
os.getcwd()


